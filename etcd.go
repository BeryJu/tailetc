// Package etcd implements a caching etcd v3 client.
//
//
// Transaction Model
//
// It presents a simplified transaction model that assumes that multiple
// writers will not be contending on keys. The high-level summary of the
// model is: when you call tx.Put(k, v), your Tx records the current
// revision of the key. If when you Commit the Tx some newer revision of
// the key has already been committed to etcd then the commit will fail.
//
// Any Tx created by the DB will always show the results of every Tx
// Commited before it on the DB (the values of external commits will be
// integrated rapidly). This means that for a single process, there will
// never be any "stale reads" across transactions.
//
// If you serve a REST API from a single *etcd.DB instance, then the API
// will behave as users expect it to.
//
// If you know only one thing about this etcd client, know this:
//
//	Do not have writer contention on individual keys.
//
// Everything else should follow a programmer's intution for a KV-store.
//
//
// Caching
//
// We take advantage of etcd's watch model to maintain a *decoded*
// value cache of the entire database in memory. This means that
// querying a value is extremely cheap. Issuing tx.Get(key) involves
// no more work than a load from a sync.Map and calling the Clone method
// on the value.
//
// The etcd watch is then exposed to the user of *etcd.DB via the
// WatchFunc. This lets users maintain in-memory higher-level indexes
// into the etcd database that respond to external commits.
//
//
// Implementation Notes
//
// Built on the gRPC JSON gateway:
// https://etcd.io/docs/v3.4.0/dev-guide/api_grpc_gateway
// This costs us in throughput and latency to etcd, while keeping gRPC
// (and its ops overhead) out of our software.
//
// As the REST API is generated from the gRPC API, this is the
// canonical source for figuring out commands:
// https://github.com/etcd-io/etcd/blob/master/etcdserver/etcdserverpb/rpc.proto
//
//
// Object Ownership
//
// The cache in etcd.DB is very careful not to copy objects both into
// and out of the cache so that users of the etcd.DB cannot get pointers
// directly into the memory inside the cache. This means over-copying.
// Users can control precisely how much copying is done, and how, by
// providing a CloneFunc implementation.
//
// The general ownership semantics are: objects are copied as soon as
// they are passed to etcd, and any memory returned by etcd is owned
// by the caller.
package etcd

import (
	"bufio"
	"bytes"
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"io/ioutil"
	"log"
	"net/http"
	"sort"
	"strings"
	"sync"
	"sync/atomic"
	"time"
)

var (
	ErrTxStale  = errors.New("tx stale")
	ErrTxClosed = errors.New("tx closed")
)

const debug = false

// DB is an in-memory cache of an etcd cluster.
type DB struct {
	url  string
	opts Options
	rev  rev // latest known etcd revision

	done        <-chan struct{}
	watchCancel func()
	shutdownWG  sync.WaitGroup

	cacheMu sync.RWMutex
	cache   map[string]kvrev
}

// Options are optional settings for a DB.
type Options struct {
	AuthHeader string
	Logf       func(format string, args ...interface{})
	HTTPC      *http.Client
	// KeyPrefix is a prefix on all etcd keys accessed through this client.
	// The value "" means "/", because etcd keys are file-system-like.
	KeyPrefix string
	// EncodeFunc encodes values for storage. If nil values must be []byte.
	EncodeFunc func(key string, value interface{}) ([]byte, error)
	// DecodeFunc decodes values from storage. If nil values are all []byte.
	DecodeFunc func(key string, data []byte) (interface{}, error)
	// CloneFunc produces a new copy of value with no aliased mutable memory.
	// The definition of "aliased mutable memory" is left to the user.
	// For example, if a user is certain that no values ever passed to or
	// read from the etcd package are ever modified, use a no-op CloneFunc.
	// If nil, the default is EncodeFunc+DecodeFunc.
	CloneFunc func(key string, value interface{}) (interface{}, error)
	// WatchFunc is called when key-value pairs change in the DB.
	// WatchFunc is called with the updated values generated by a Tx
	// before Commit returns on that Tx.
	WatchFunc func([]KV)
}

func (opts Options) fillDefaults() Options {
	if opts.HTTPC == nil {
		opts.HTTPC = http.DefaultClient
	}
	if opts.KeyPrefix == "" {
		opts.KeyPrefix = "/"
	}
	if opts.Logf == nil {
		opts.Logf = log.Printf
	}
	if opts.EncodeFunc == nil {
		opts.EncodeFunc = func(key string, value interface{}) ([]byte, error) {
			if value == nil {
				return nil, nil
			}
			b, isBytes := value.([]byte)
			if !isBytes {
				return nil, fmt.Errorf("etcd: default EncodeFunc requires all values be []byte")
			}
			b2 := make([]byte, len(b))
			copy(b2, b)
			return b2, nil
		}
	}
	if opts.DecodeFunc == nil {
		opts.DecodeFunc = func(key string, data []byte) (interface{}, error) {
			return data, nil
		}
	}
	if opts.CloneFunc == nil {
		opts.CloneFunc = func(key string, value interface{}) (interface{}, error) {
			data, err := opts.EncodeFunc(key, value)
			if err != nil {
				return nil, fmt.Errorf("clone: %w", err)
			}
			cloned, err := opts.DecodeFunc(key, data)
			if err != nil {
				return nil, fmt.Errorf("clone: %w", err)
			}
			return cloned, nil
		}
	}
	return opts
}

// KV is a key-value pair with the value decoded. It is used by WatchFunc.
type KV struct {
	Key   string
	Value interface{}
}

// New loads the contents of an etcd prefix range and creates a *DB
// for reading and writing from the prefix range.
func New(loadCtx context.Context, url string, opts Options) (*DB, error) {
	db := &DB{
		url:   url,
		opts:  opts.fillDefaults(),
		cache: make(map[string]kvrev),
	}
	if err := db.loadAll(loadCtx); err != nil {
		return nil, fmt.Errorf("etcd.New: could not load: %w", err)
	}

	watchCtx, cancel := context.WithCancel(context.Background())
	db.done = watchCtx.Done()
	db.shutdownWG.Add(1)
	go func() {
		defer db.shutdownWG.Done()
		for {
			if watchCtx.Err() != nil {
				return
			}
			if err := db.watch(watchCtx); err != nil {
				if errors.Is(err, context.Canceled) {
					return
				}
				db.logf("etcd.watch: %v", err)
			}
			t := time.NewTimer(100 * time.Millisecond)
			select {
			case <-watchCtx.Done():
				t.Stop()
				return
			case <-t.C:
			}
		}
	}()
	db.watchCancel = cancel

	return db, nil
}

// ReadTx create a new read-only transaction.
func (db *DB) ReadTx() *Tx {
	return &Tx{ro: true, db: db}
}

// Tx creates a new database transaction.
func (db *DB) Tx(ctx context.Context) *Tx {
	return &Tx{ctx: ctx, db: db}
}

func (db *DB) Close() error {
	select {
	case <-db.done:
		return errors.New("etcd.DB: already closed")
	default:
	}
	db.watchCancel()
	db.shutdownWG.Wait()
	return nil
}

// A Tx is an etcd transaction.
//
// A Tx holds no resources besides some private memory, so there is
// no notion of closing a transaction or rolling back a transaction.
// A cheap etcd read can be done with:
//
//	val, err := db.ReadTx().Get(key)
type Tx struct {
	ctx    context.Context
	err    error // if set, tx no longer usable
	db     *DB
	ro     bool // readonly
	maxRev rev
	cmps   map[string]struct{}
	puts   map[string]interface{}
}

// Get gets a KV-pair from the etcd cache. It does not use the network.
//
// The first call to Get in a Tx will pin the revision number to the
// current etcd revision. If a subsequent Get finds a value with a
// greater revision then get will return ErrTxStale. This ensures that
// a Tx has a consistent view of the values it fetches.
func (tx *Tx) Get(key string) (interface{}, error) {
	kv, err := tx.get(key)
	if err != nil {
		return nil, fmt.Errorf("etcd.Get(%q): %w", key, err)
	}
	cloned, err := tx.db.opts.CloneFunc(key, kv.value)
	if err != nil {
		return nil, fmt.Errorf("etcd.Get(%q): %w", key, err)
	}
	return cloned, nil
}

// GetRange gets a range of KV-pairs from the etcd cache.
func (tx *Tx) GetRange(keyPrefix string, fn func([]KV)) (err error) {
	var kvs []KV

	// TODO(crawshaw): This is an inefficient O(N) implementation.
	// We can make this in-memory efficient by storing an ordered tree of keys.
	// Or we can factor out the load method and use etcd's /range with keys_only=true.
	tx.db.cacheMu.RLock()
	defer tx.db.cacheMu.RUnlock()
	for key := range tx.db.cache {
		if !strings.HasPrefix(key, keyPrefix) {
			continue
		}
		kv, err := tx.get(key)
		if err != nil {
			return fmt.Errorf("etcd.GetRange(%q): %s: %w", keyPrefix, key, err)
		}
		v, err := tx.db.opts.CloneFunc(key, kv.value)
		if err != nil {
			return fmt.Errorf("etcd.GetRange(%q): clone %s: %w", keyPrefix, key, err)
		}
		kvs = append(kvs, KV{Key: key, Value: v})
		if len(kvs) > 100 {
			fn(kvs)
			kvs = nil
		}
	}

	if err == nil && len(kvs) > 0 {
		fn(kvs)
		kvs = nil
	}
	return err
}

// Put adds or replaces a KV-pair in the transaction.
// If a newer value for the key is in the DB this will return ErrTxStale.
func (tx *Tx) Put(key string, value interface{}) error {
	if _, err := tx.get(key); err != nil {
		return fmt.Errorf("etcd.Put(%q): %w", key, err)
	}
	if tx.ro {
		tx.err = fmt.Errorf("etcd.Put(%q) called on read-only transaction", key)
		return tx.err
	}
	if tx.puts == nil {
		tx.puts = make(map[string]interface{})
	}
	cloned, err := tx.db.opts.CloneFunc(key, value)
	if err != nil {
		tx.err = fmt.Errorf("etcd.Put(%q): %w", key, err)
		return tx.err
	}
	tx.puts[key] = cloned
	return nil
}

// Commit commits the transaction to etcd.
// It is an error to call Commit on a read-only transaction.
func (tx *Tx) Commit() error {
	if tx.err != nil {
		return fmt.Errorf("etcd.Commit: %w", tx.err)
	}
	defer func() {
		tx.err = ErrTxClosed
	}()
	if tx.ro {
		return fmt.Errorf("etcd.Commit: tx is read-only")
	}
	if len(tx.puts) == 0 {
		return nil
	}

	tx.db.shutdownWG.Add(1)
	defer tx.db.shutdownWG.Done()
	ctx, cancel := context.WithCancel(tx.ctx)
	defer cancel()
	go func() {
		select {
		case <-tx.db.done:
			// db.Close called, cancel commit
		case <-ctx.Done():
			// tx.Commit complete, clean up this goroutine
		}
		cancel()
	}()

	type txnCompare struct {
		ModRevision rev    `json:"mod_revision"`
		Result      string `json:"result"`
		Target      string `json:"target"`
		Key         []byte `json:"key"`
	}
	type txnSuccess struct {
		RequestPut struct {
			Key   []byte `json:"key"`
			Value []byte `json:"value"`
		} `json:"requestPut"`
	}
	var txnReq struct {
		Compare []txnCompare `json:"compare"`
		Success []txnSuccess `json:"success"`
	}
	for key := range tx.cmps {
		txnReq.Compare = append(txnReq.Compare, txnCompare{
			ModRevision: tx.maxRev + 1,
			Result:      "LESS",
			Target:      "MOD",
			Key:         []byte(key),
		})
	}
	for key, val := range tx.puts {
		data, err := tx.db.opts.EncodeFunc(key, val)
		if err != nil {
			return fmt.Errorf("etcd.Commit: EncodeFunc failed for key %q: %v", key, err)
		}

		var s txnSuccess
		s.RequestPut.Key = []byte(key)
		s.RequestPut.Value = data
		txnReq.Success = append(txnReq.Success, s)
	}
	data, err := json.Marshal(txnReq)
	if err != nil {
		return fmt.Errorf("etcd.Commit: %w", err)
	}
	if debug {
		tx.db.opts.Logf("commit req: %s", data)
	}

	req, err := http.NewRequest("POST", tx.db.url+"/v3/kv/txn", bytes.NewReader(data))
	if err != nil {
		return fmt.Errorf("etcd.Commit: %w", err)
	}
	req = req.WithContext(ctx)
	res, err := tx.db.opts.HTTPC.Do(req)
	if err != nil {
		return fmt.Errorf("etcd.Commit: %w", err)
	}
	defer res.Body.Close()
	if res.StatusCode != 200 {
		b, _ := ioutil.ReadAll(res.Body)
		return fmt.Errorf("etcd.Commit: status=%d: %q", res.StatusCode, string(b))
	}
	var txnRes struct {
		Header struct {
			Revision rev `json:"revision"`
		} `json:"header"`
		Succeeded bool `json:"succeeded"`
		// TODO(crawshaw): the "responses" array contains a revision value
		// for each request put, do these values ever differ from the outer
		// revision on success?
	}
	b, err := ioutil.ReadAll(res.Body)
	if err != nil {
		panic(err)
	}
	if debug {
		tx.db.opts.Logf("commit res: %s", b)
	}
	if err := json.NewDecoder(bytes.NewReader(b)).Decode(&txnRes); err != nil {
		return fmt.Errorf("etcd.Commit: response: %w", err)
	}
	if !txnRes.Succeeded {
		return ErrTxStale
	}

	// Immediately load the new values into the cache.
	// In theory the watch will fill in these values shortly, but
	// doing it here has two advantages:
	//	1. any Tx started immediately after this Tx will see the new
	//	   values, avoiding "stale" reads in the API
	//	2. it avoids significant JSON-decoding in the watch goroutine
	tx.db.cacheMu.Lock()
	defer tx.db.cacheMu.Unlock()
	for key, val := range tx.puts {
		kv, exists := tx.db.cache[key]
		if exists {
			if txnRes.Header.Revision <= kv.modRev {
				delete(tx.puts, key)
				continue
			}
		}
		tx.db.cache[key] = kvrev{
			value:  val,
			modRev: txnRes.Header.Revision,
		}
	}
	if tx.db.opts.WatchFunc != nil && len(tx.puts) > 0 {
		var kvs []KV
		for key, val := range tx.puts {
			cloned, err := tx.db.opts.CloneFunc(key, val)
			if err != nil {
				// This is an unexpected failure.
				// By this point, we know val is the output of CloneFunc
				// called earlier in Tx.Put. That CloneFunc fails on
				// the value's second pass through suggests a bug in
				// the implementation of CloneFunc.
				tx.db.logf("[unexpected] etcd tx watch second clone of %q failed: %w", key, err)
				continue
			}
			kvs = append(kvs, KV{Key: key, Value: cloned})
		}
		sort.Slice(kvs, func(i, j int) bool { return kvs[i].Key < kvs[j].Key })
		tx.db.opts.WatchFunc(kvs)
	}

	return nil
}

type kvrev struct {
	value  interface{} // json-decoded value or json.RawMessage if prefix unknown
	modRev rev         // mod_revision
}

type rev int64

func (r rev) MarshalText() (text []byte, err error) {
	return []byte(fmt.Sprintf("%d", int64(r))), nil
}
func (r *rev) UnmarshalText(text []byte) error {
	_, err := fmt.Sscanf(string(text), "%d", (*int64)(r))
	return err
}

func (db *DB) logf(format string, args ...interface{}) {
	db.opts.Logf(format, args...)
}

func (db *DB) watch(ctx context.Context) error {
	var watchRequest struct {
		CreateRequest struct {
			Key           []byte `json:"key"`
			RangeEnd      []byte `json:"range_end"`
			StartRevision int64  `json:"start_revision"`
		} `json:"create_request"`
	}
	watchRequest.CreateRequest.Key = []byte(db.opts.KeyPrefix)
	watchRequest.CreateRequest.RangeEnd = addOne([]byte(db.opts.KeyPrefix))
	watchRequest.CreateRequest.StartRevision = atomic.LoadInt64((*int64)(&db.rev))
	data, err := json.Marshal(watchRequest)
	if err != nil {
		return err
	}
	req, err := http.NewRequest("POST", db.url+"/v3/watch", bytes.NewReader(data))
	if err != nil {
		return err
	}
	req = req.WithContext(ctx)
	res, err := db.opts.HTTPC.Do(req)
	if err != nil {
		return err
	}
	defer res.Body.Close()
	if res.StatusCode != 200 {
		b, _ := ioutil.ReadAll(res.Body)
		return fmt.Errorf("status=%d: %q", res.StatusCode, string(b))
	}

	scanner := bufio.NewScanner(res.Body)
	for scanner.Scan() {
		if err := db.watchResult(scanner.Bytes()); err != nil {
			return err
		}
	}
	if err := scanner.Err(); err != nil {
		return err
	}
	return nil
}

func (db *DB) watchResult(data []byte) error {
	var watchResult struct {
		Result struct {
			Header struct {
				Revision rev `json:"revision"`
			} `json:"header"`
			Events []struct {
				KV struct {
					Key         []byte `json:"key"`
					ModRevision rev    `json:"mod_revision"`
					Value       []byte `json:"value"`
				} `json:"kv"`
			} `json:"events"`
		} `json:"result"`
	}
	if err := json.Unmarshal(data, &watchResult); err != nil {
		return err
	}

	type newkv struct {
		key   string
		kvrev kvrev
	}
	var newkvs []newkv

	for _, ev := range watchResult.Result.Events {
		key := string(ev.KV.Key)

		// As a first pass, we check the cache to see if we can avoid decoding
		// the value. This is a performance optimization, it's entirely possible
		// the Tx commiting these values is still in-flight and will update the
		// db.cache momentarily, so it is checked again below under the mutex.
		db.cacheMu.RLock()
		kv, exists := db.cache[key]
		db.cacheMu.RUnlock()

		if exists {
			if ev.KV.ModRevision <= kv.modRev {
				// We already have this value.
				continue
			}
		}

		v, err := db.opts.DecodeFunc(key, ev.KV.Value)
		if err != nil {
			db.logf("etcd.watch: bad decoded value for key %q: %v", key, err)
			v = ev.KV.Value
		}
		newkvs = append(newkvs, newkv{
			key: key,
			kvrev: kvrev{
				value:  v,
				modRev: ev.KV.ModRevision,
			},
		})
	}

	db.cacheMu.Lock()
	var kvs []KV
	for _, newkv := range newkvs {
		if kv, exists := db.cache[newkv.key]; exists {
			if newkv.kvrev.modRev <= kv.modRev {
				// Value has just been updated by a Tx, keep newer value.
				continue
			}
		}
		if db.opts.WatchFunc != nil {
			cloned, err := db.opts.CloneFunc(newkv.key, newkv.kvrev.value)
			if err != nil {
				db.logf("[unexpected] watch clone of %q failed: %w", newkv.key, err)
				continue
			}
			kvs = append(kvs, KV{Key: newkv.key, Value: cloned})
		}
		db.cache[newkv.key] = newkv.kvrev
	}
	if len(kvs) > 0 {
		sort.Slice(kvs, func(i, j int) bool { return kvs[i].Key < kvs[j].Key })
		db.opts.WatchFunc(kvs)
	}
	db.cacheMu.Unlock()

	atomic.StoreInt64((*int64)(&db.rev), int64(watchResult.Result.Header.Revision))
	return nil
}

func (db *DB) loadAll(ctx context.Context) error {
	db.cacheMu.Lock()
	defer db.cacheMu.Unlock()

	startKey := []byte(db.opts.KeyPrefix)
	endKey := addOne([]byte(db.opts.KeyPrefix))
	var maxModRev rev
	for len(startKey) > 0 {
		nextKey, dbRev, err := db.load(ctx, startKey, endKey, maxModRev)
		if err != nil {
			return err
		}
		if maxModRev == 0 {
			maxModRev = dbRev
		}
		startKey = nextKey
	}
	return nil
}

const loadPageLimit = 100

func (db *DB) load(ctx context.Context, startKey, endKey []byte, maxModRev rev) (nextKey []byte, dbRev rev, err error) {
	var keyRange struct {
		Key            []byte `json:"key"`
		RangeEnd       []byte `json:"range_end,omitempty"`
		Limit          int    `json:"limit,omitempty"`
		MaxModRevision int64  `json:"max_mod_revision,omitempty"`
	}
	keyRange.Key = startKey
	keyRange.RangeEnd = endKey
	keyRange.Limit = loadPageLimit
	keyRange.MaxModRevision = int64(maxModRev)
	keyRangeData, err := json.Marshal(keyRange)
	if err != nil {
		return nil, 0, err
	}
	req, err := http.NewRequest("POST", db.url+"/v3/kv/range", bytes.NewReader(keyRangeData))
	if err != nil {
		return nil, 0, err
	}
	req = req.WithContext(ctx)
	res, err := db.opts.HTTPC.Do(req)
	if err != nil {
		return nil, 0, err
	}
	defer res.Body.Close()
	if res.StatusCode != 200 {
		b, _ := ioutil.ReadAll(res.Body)
		return nil, 0, fmt.Errorf("etcd.New: key range status=%d: %q", res.StatusCode, string(b))
	}

	var rangeResponse struct {
		Header struct {
			Revision rev `json:"revision"`
		} `json:"header"`
		KVs []struct {
			ModRevision rev    `json:"mod_revision"`
			Key         []byte `json:"key"`
			Value       []byte `json:"value"`
		} `json:"kvs"`
		More bool `json:"more"`
	}
	if err := json.NewDecoder(res.Body).Decode(&rangeResponse); err != nil {
		return nil, 0, err
	}
	dbRev = rangeResponse.Header.Revision

	var kvs []KV
	for _, kv := range rangeResponse.KVs {
		key := string(kv.Key)
		v, err := db.opts.DecodeFunc(key, kv.Value)
		if err != nil {
			db.logf("%q: cannot decode: %v", key, err)
			continue
		}
		db.cache[key] = kvrev{
			value:  v,
			modRev: kv.ModRevision,
		}
		cloned, err := db.opts.CloneFunc(key, v)
		if err != nil {
			db.logf("[unexpected] %q clone of decoded value failed: %w", key, err)
			continue
		}
		kvs = append(kvs, KV{Key: key, Value: cloned})
	}
	if db.opts.WatchFunc != nil && len(kvs) > 0 {
		db.opts.WatchFunc(kvs)
	}

	if rangeResponse.More {
		nextKey = addOne([]byte(rangeResponse.KVs[len(rangeResponse.KVs)-1].Key))
	}
	return nextKey, dbRev, nil
}

func addOne(v []byte) []byte {
	for len(v) > 0 && v[len(v)-1] == 0xff {
		v = v[:len(v)-1]
	}
	if len(v) > 0 {
		v[len(v)-1]++
	}
	return v
}

func (tx *Tx) get(key string) (kvrev, error) {
	if !strings.HasPrefix(key, tx.db.opts.KeyPrefix) {
		return kvrev{}, fmt.Errorf("key does not use prefix %s", tx.db.opts.KeyPrefix)
	}
	if tx.err != nil {
		return kvrev{}, tx.err
	}

	tx.db.cacheMu.RLock()
	kv, ok := tx.db.cache[key]
	tx.db.cacheMu.RUnlock()

	if !ok {
		return kvrev{}, nil
	}
	if tx.maxRev == 0 {
		// reading from rev races with the watch writing to cache,
		// so when setting maxRev: first read db.rev, and then load
		// the cache value, and choose the greater of the two.
		tx.maxRev = rev(atomic.LoadInt64((*int64)(&tx.db.rev)))
		if tx.maxRev < kv.modRev {
			tx.maxRev = kv.modRev
		}
	} else if tx.maxRev < kv.modRev {
		return kvrev{}, ErrTxStale
	}
	if tx.cmps == nil {
		tx.cmps = make(map[string]struct{})
	}
	tx.cmps[key] = struct{}{}
	return kv, nil
}

// TODO(crawshaw): type Key string ?
// TODO(crawshaw): Delete
// TODO(crawshaw): Watch Delete
